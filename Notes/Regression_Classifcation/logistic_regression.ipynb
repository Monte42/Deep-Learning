{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression is a way to model the probability of outcome given an input.\\\n",
    "Most cases, Logistic Regression will model a binary outcome. Such as: (Yes,No), (True,False), etc.\\\n",
    "We know how to find the best fitting line with [Linear Regression](./simple_linear_regression.ipynb)\\\n",
    "That works well when the y-axis can range from -n to n, but what if the out was binary? 1 or 0\\\n",
    "Lets look and see who linear regression works for that.\\\n",
    "![img](./img/logistic_regression(1).png)\\\n",
    "It kind of works, but after looking at the graph, can you see a better correlation?\\\n",
    "Lets this is a dataset from an insurance company.\\\n",
    "They are running a deal on life insurance, and the are sending email to customers with the offer.\\\n",
    "These are the results gathered so far.\\\n",
    "We have the customers age and if they accepted the offer or not.\\\n",
    "\\\n",
    "We can determin that the dependent variable is if the accept the offer and the independent variable is the age.\\\n",
    "We the y-axis goes from 0 to 1, or yes and no, and the x-axis starts at 0.\\\n",
    "Lets say that the age.\\\n",
    "We can see as we are closer to 0 on the x-axis the more likely that observation will be 0 on the y-axis.\\\n",
    "The further from 0 on the x-axis, the more likely that observation will be 1 on the y-axis.\\\n",
    "So the older a person is the more likely they are to take the offer.\\\n",
    "We can also say any observation left of the linear lines intersection of the x-axis is guaranteed to say no.\\\n",
    "While any observation right of the linear lines intersection of 1 on the y-axis are guaranteed yes.\\\n",
    "Most importantly we can say that there aren't any y values outside 1 or 0, so there is no use in computing trends beyond thoes points.\\\n",
    "This is where Logistic Regression comes in.\\\n",
    "\n",
    "### How Logistic Regression Works\n",
    "So up until now we have been thinking about this in the sense of [Linear Regression](./simple_linear_regression.ipynb).\\\n",
    "Which is part of the equation, literally!\\\n",
    "What we need to do is apply the Linear Regression formula to the [Sigmoid Function](../Activation%20Functions/sigmoid.ipynb).\\\n",
    "Lets have a look at them both now as a refresher.\n",
    "> Linear Regression\n",
    "> #### $y=b_{0}+b_{1}*x_{1}$\n",
    "> Sigmoid Function\n",
    "> #### $p=\\frac{1}{1+e^{-y}}$\n",
    "So whats going on here is the the linear regression formula gives us our best trend line \"y\".\\\n",
    "We then solve for $y$ in the Sigmoid function with the result of the of the Linear Regression.\\\n",
    "That will look something like this.\n",
    "> #### $ln(\\frac{p}{1-p})=b_{0}+b_{1}*x_{1}$\n",
    "> ###### $ln(x)$ is [Natural Logarithm](https://betterexplained.com/articles/demystifying-the-natural-logarithm-ln/) \n",
    "The Sigmoid will \"Compress\" our trend line to fit the dataset.\\\n",
    "That will ultimately end up looking like this.\\\n",
    "![img](./img/logistic_regression(2).png)\\\n",
    "Now we can see that our prediction model better fits out dataset.\\\n",
    "With our better fitting model, we can get more accurate probabilties if the customer will say yes.\\\n",
    "If we draw a line from an observation to the sigmoid line then from there over to the y-axis we can see the probablity that customer will take the offer.\\\n",
    "![img](./img/logistic_regression(3).png)\\\n",
    "(Not to Scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "At this point if we are given a new customers age we can apply age to our train model.\\\n",
    "We find the age on the x-axis and drawn a line to the the sigmoid line and back over to the y-axis to get out probablity ( $\\hat{p}$ )\n",
    "Now:\n",
    "- If $\\hat{p}<0.5$: we predict the customer with say no.\n",
    "- If $\\hat{p}>=0.5$: we predict the customer with say yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 0.5 is only a commonly use metric.  If you need a tight acceptance probability you can raise it.\\\n",
    "As said before Logistic Reggression is most commonly use for models with binary outputs.\\\n",
    "Still they sigmoid returns a probability, so there are other uses to the Logistic Regression."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
