{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How The Neural Network Works\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Forward propagation is when an observation is passed through the model.\\\n",
    "As we already know, a NN would consist of an input layer, hidden layer, and an output layer.\\\n",
    "We also know that the diffent input values, or Independent Variables, are assigned weights,\\\n",
    "and these weights affect the influence that independent variable will have on the models output, or Dependent Variable.\\\n",
    "\\\n",
    "Lets look at another example.\\\n",
    "\\\n",
    "![forward_prop](./img/forward_prop.png)\\\n",
    "\\\n",
    "Looking at the example above, we can see that only certain independent variables are being passed to specific nodes.\\\n",
    "This is because ever connection between nodes will get its own weight, and some weights may be 0.\\\n",
    "\\\n",
    "Lets say this models job is to predict the price of a pair of sneakers, and the four parameters are brand, model, year, and size.\\\n",
    "During training the model may find better results when the first node of the hidden layer receives brand and year.\\\n",
    "While the second node receives brand and size, and so on.\\\n",
    "\\\n",
    "If the model is not learning or training, these values won't be changed.\\\n",
    "This is because the model only back propagates when its learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backwards Propagation\n",
    "After the forward the propagation the model will need to compare its output( $\\hat{y}$ ) to the real world value( $y$ ).\\\n",
    "$\\hat{y}$, y hat is the predicted or probable outcome from the model, while $y$ is the actual recorded value from the dataset.\\\n",
    "In our above example, every row, or sneaker would have a fifth column, price, this is our dependent variable.\\\n",
    "The model will need to take the its predictced value and compare it to the real value of the sneaker.\\\n",
    "\\\n",
    "We can find the difference using what is called a cost function.\\\n",
    "A cost fuction is like an activation function, just a simple computation.\\\n",
    "There are many pre developed cost functions to choose from, a commmon used one is $c=\\frac{1}{2}(\\hat{y}-y)^{2}$.\\\n",
    "This difference is called the loss.\\\n",
    "\\\n",
    "![back_prop](./img/backwards_prop.png)\\\n",
    "\\\n",
    "The lower the loss the closer the models predicted value is to the actual value.\n",
    "The loss, typically a negative value and not 0, will be pass back through the model and the all the weights will be updated.\\\n",
    "The model will run again, repeating the process until the model converges at its best loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting It All Together\n",
    "So far we have been working with a single row, or observation, and a single pass through the model.\\\n",
    "Typically, but not always, we will break our train data into batches.\\\n",
    "Each batch containing multiple observations.\\\n",
    "One by one these obsrvations are passed through the model, and predicted values are stored.\\\n",
    "At the end on the batch, our model will calculate the Sum of 1/2 of all the predicted values minus the actual values squared.\n",
    "> #### $C=\\sum\\frac{1}{2}(\\hat{y}-y)^{2}$\n",
    "This new loss, which is calculated from the whole batch, is then back propagated.\\\n",
    "\\\n",
    "Every one of these full cycles of both forward and backward propgation is called an epoch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
