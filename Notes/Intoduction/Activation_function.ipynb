{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "---\n",
    "As mentioned in the previous section, it is common practice to pass a nodes output through an activation function.\\\n",
    "An activation is a second computation preformed on the data returned from the node.\\\n",
    "There are many types of avtivation function, below are 4 of the more common examples.\\\n",
    "\\\n",
    "In this section will be just be a high level introduction to each of the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold\n",
    "![threshold](./img/threshold.png)\n",
    "> #### $\\phi(x)=\\binom{1 if \\ge 0}{0 if < 0}$\n",
    "Just to clear it up, x is the output of the node and $\\phi()$ is the current activation function.\\\n",
    "So $\\phi(x)$ is saying this activation applied to the nodes output.\\\n",
    "\\\n",
    "In short the Threshold function is going to return either a 1 or a 0.\\\n",
    "It will return 1 if the output is 0 or greater than 0.\\\n",
    "If the output is less than 0, the Threshold function will return 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "![sigmoid](./img/sigmoid.png)\n",
    "> #### $\\phi(x)=\\frac{1}{1+e^{-x}}$\n",
    "The Sigmoid function, also know as the squashing function, will return a value between 0 and 1.\\\n",
    "Unlike the Threshold function, its not limited to just 1 or 0, but any value between those two points.\\\n",
    "These values are the probability that node is true.\\\n",
    "For intsance, if our model has two output nodes, lets say cat or dog.\\\n",
    "Each nodes value represents the agents certainty of that node being correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectifier\n",
    "![rectifier](./img/rectifier.png)\n",
    "> #### $\\phi(x)=max(x,0)$\n",
    "Although simple in its way, the Rectifier funtion, I feel, is one of the most popular activation functions.\\\n",
    "This activation funtion will just simply take the output and if it is less than 0, it will return 0.\\\n",
    "Otherwise, the origin output value will be returned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic Tangent\n",
    "![hyperbolic](./img/hyperbolic.png)\n",
    "> #### $\\phi(x)=\\frac{1-e^{-2x}}{1+e^{-2x}}$\n",
    "The Hyperbolic Tangent fuction follows the same pattern as the Sigmoid function.\\\n",
    "The key difference is that instead of compressing the the outputed values to be between 0 and 1,\\\n",
    "they are compressed to values between -1 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Note\n",
    "These are just a few of the many activation function available.\\\n",
    "We can even create our own custom activation functions.\\\n",
    "\\\n",
    "Another thing to add, is that in the previous section [The Neuron](The_Neuron.ipynb), we only applied the **Hidden Layer**.\\\n",
    "Its is very common to apply an activation function to both the **Hidden Layer**(s) and the **Output Layer**.\\\n",
    "A common practice seen it the Rectifier function applied to the node and the Sigmoid function applied\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
