{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artrficial Neural Networks\n",
    "---\n",
    "In the dataset for this project we have a list of customers to a bank.\\\n",
    "The bank has noticed a recent rise customers leaving the bank.\\\n",
    "They are requesting a system that can predict if a customer is likely to leave as a customer.\\\n",
    "\n",
    "**TABLE COLUMNS:**\\\n",
    "RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Needed Packages\n",
    "- Pandas\n",
    "- NumPy\n",
    "- TenserFlow\n",
    "- scikit-learn\n",
    "\n",
    "```\n",
    "pip install pandas numpy tenserflow scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports\n",
    "The only thing to add to the block is a little more detail about the Enoders.\\\n",
    "LableEncoder is a tool from scikit-learn that will give each individual instance of a string an integer value,\n",
    "the strings in the already existing column will be replaced with these assigned integers.\\\n",
    "OneHotEncoder is also a scikit-learn tool for handeling categorical data.\\\n",
    "As an example, if a column, 'Color', contained the possible strings of ('red','blue','green').\\\n",
    "When used with ColumnTransformer, the 'color' column will be removed, three new column will be inserted, a 1 will be placed in the respective colums,\\\n",
    "otherwise the column will receive a 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # To work with data files like .csv\n",
    "import numpy as np # For handeling and manipulating data\n",
    "import tensorflow as tf # To build the learning model\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler # Binary encoding, Multiclass encoding, Feature scaler\n",
    "from sklearn.compose import ColumnTransformer # Changes Multiclass column into multiple columns & inserts values\n",
    "from sklearn.model_selection import train_test_split # Splits data into two pairs of train and test sets\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score # To see model results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Before we pass the data to the model we need to work the data a bit.\\\n",
    "Right now as it is we have columns in the table that aren't needed.\\\n",
    "We also have columns that contain strings.\\\n",
    "The model can't work with strings, so this will need to be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset\n",
    "To import the data, we use Pandas.\\\n",
    "Pandas will return what is called a dataframe.\\\n",
    "Panda dataframes are like arrays, but are not arrays.\\\n",
    "They have their prebuilt methods for indexing and parsing data.\\\n",
    "\\\n",
    "We are going to start with importing the data, then parsing out only the data we need.\\\n",
    "Keep in mind we are looking input features and outputs.\\\n",
    "The bank wants to know if someone is likely to \"Exit\" the bank, so we know the last column are the output we want our model to predict.\\\n",
    "We may also imply that RowNumber, CustomerId, and Surname wouldn't be relevant in a customers decison on leaving the bank.\\\n",
    "So we will collect all the columns, minus the \"outputs\", and the independent variables \"X\".\\\n",
    "Then fetch the last column as the dependent variables \"y\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.csv') # Imports Data\n",
    "\n",
    "X = df.iloc[:,3:-1].values # Parses out [All Rows, 3rd to 2nd last column]\n",
    "y = df.iloc[:,-1].values # Parses out [All Rows, last column only]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Data\n",
    "Now we need to handle the categorical data, or data that is not represented by an interger or float.\\\n",
    "Fist we'll handle the gender column, which only contains 2 individual strings.\\\n",
    "\\\n",
    "For this we will use the LableEncoder class from scikit-learn.\\\n",
    "We create an instance of the LabelEncoder class.\\\n",
    "Then we use the class instance to encode the column with an index of 2, all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder() \n",
    "X[:,2] = le.fit_transform(X[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we handle the \"Geography\" column.\\\n",
    "This time we will use ColumnTransformer with OneHotEncoder.\\\n",
    "How this affect the dataset was explained above, what we'll cover here are the arguments we are passing the ColumnTransformer.\\\n",
    "\\\n",
    "The first argument transformers, will receive an array of tuples, each tuple being a different encoder.\\\n",
    "This first element in the tuple is the name of the encoder. It can be anything, this is for referencing later.\\\n",
    "The second element is the encoding function we intend to use,\\\n",
    "The third element of the tuple is the column index we wish to encode.\\\n",
    "\\\n",
    "The second argument remainder, is what the ColumnTransformer will do with all the column not being encoded.\\\n",
    "By default it is set to drop, but in this case we want to keep the data so we set it to passthrough,\\\n",
    "this will leave the rest of the column as they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough') # transformer name, encoding func, col to encode\n",
    "X = np.array(ct.fit_transform(X)) # we convert the data to a numpy array because the next step will be expecting numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Into Train\\Test Sets\n",
    "Now that we made sure all the data is either an int or float we can split the data into 2 sets.\\\n",
    "One set for training and the second, unseen to the model durning training, to test with.\\\n",
    "\\\n",
    "As we aleardy know, we have two sets of data, the input features and the outputs.\\\n",
    "So it will make sense that they both need to be split into train and test sets.\\\n",
    "When doing so we need to make sure that we keep the right outputs inline with the input features.\\\n",
    "Scikit-Learn provides us a tool for that, train_test_split.\\\n",
    "\\\n",
    "train_test_split returns 4 numpy arrays in this order: *input-train, input-test, output-train, output-test*\n",
    "For arguments, only the first 2 are required.\\\n",
    "First being the input-feature, independent variables, or \"X\".\\\n",
    "Second being the output, dependent variables, or \"y\".\\\n",
    "Train_size is the percent value of the data you want to save for testing, it defaults to 0.2, which is a common starting point.\\\n",
    "\\\n",
    "When selecting data for the test sets, rows are picked at random.\\\n",
    "Setting random_state=0 will mean it the rows selected are the always the same.\\\n",
    "This is for us humans to learn easier, normally this will not be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By setting random_state to 0 we will get the train test split every time, for human learning. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.2, random_state=0) # input features, expected outputs, % of data for test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data\n",
    "The last thing we have to do with the data is scale it.\\\n",
    "Right now some values are small like 0, 1, or 2, while others are larger, 1000's or even 10's of 1000's.\\\n",
    "We don't want these larger values to have any more influence on the models decision than any other column.\\\n",
    "So we will put them in a more comparable scale.\\\n",
    "\\\n",
    "For this we will use the StandaredScaler class.\\\n",
    "This will take all and convert it into much smaller positive and negative values.\\\n",
    "More importantly, these new values assigned are all based on the scale, giving all the columns the same influence on the model.\\\n",
    "\\\n",
    "We will want to make sure we scale both the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Artificial Neural Network\n",
    "With Tensorflow we have access to a sub-package called keras, which we will use to build the neural network.\\\n",
    "We'll start with initiating our model with the keras.Sequential() model.\\\n",
    "The Sequential model is appropriate when each layer will have only one input/output tensor.\\\n",
    "\\\n",
    "Once the model has been initiated, we can start adding layers using Sequential's add method.\\\n",
    "Again we turn to keras to construct the network layers.\\\n",
    "With keras.layers we have access to the Dense layer, which is just a regual densely-connected NN layer.\\\n",
    "When adding a Dense layer to the network, we only need 2 arguments.\\\n",
    "Units which is the number of nodes in that layer.\\\n",
    "Activation, is short for activation function.\\\n",
    "\\\n",
    "When building the NN we need to make sure that units of the first layer match the number features we have in our dataset.\\\n",
    "Then we also want to make sure that last layer macthes the number of outputs.\\\n",
    "Outside of that the number of layers and nodes is based on the neccessity of the problem, and knowing when to start comes with experience.\n",
    "\\\n",
    "Last thing we do is compile the model, which is configure the model for training.\\\n",
    "All the parameters have preset values, but typically we will set the optimizer and cost fucntion.\\\n",
    "If we want to get any specific metrics, we can set that now too.\\\n",
    "\\\n",
    "The Adam optimization algorithm is a widely used optizer that is an extension to stochastic gradient descent.\\\n",
    "Binary_crossentropy is a cost function good for caluclation errors in binary solutions.\\\n",
    "When we train the model it will automatically print the loss, by adding 'accuracy' to the metrics, we will get that too.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tf.keras.Sequential()\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu')) # Input Layer / 6 nodes / rectifier activation func\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu')) # Hidden Layer / 6 nodes / rectifier activation func\n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid')) # Output Layer / 6 nodes / sigmoid activation func\n",
    "ann.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train The Neural Network\n",
    "Simply put, .fit() will \"fit\" the model the training data.\\\n",
    "So lets cover the arguments.\\\n",
    "\\\n",
    "First will be the training input features(\"X\"), and the second, training outputs(\"y\").\\\n",
    "Batch size is tell the model how many rows to randomly select and pass through the model.\\\n",
    "Epochs is how many iterations of passing batches too, and updating the model will take place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 0.6337 - accuracy: 0.6765\n",
      "Epoch 2/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.5308 - accuracy: 0.7960\n",
      "Epoch 3/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4998 - accuracy: 0.7995\n",
      "Epoch 4/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.7995\n",
      "Epoch 5/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4771 - accuracy: 0.7995\n",
      "Epoch 6/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.7985\n",
      "Epoch 7/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4642 - accuracy: 0.7985\n",
      "Epoch 8/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8000\n",
      "Epoch 9/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4547 - accuracy: 0.8005\n",
      "Epoch 10/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.8010\n",
      "Epoch 11/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4478 - accuracy: 0.8015\n",
      "Epoch 12/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8020\n",
      "Epoch 13/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4419 - accuracy: 0.8035\n",
      "Epoch 14/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4392 - accuracy: 0.8035\n",
      "Epoch 15/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.8040\n",
      "Epoch 16/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4342 - accuracy: 0.8045\n",
      "Epoch 17/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4321 - accuracy: 0.8050\n",
      "Epoch 18/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4295 - accuracy: 0.8075\n",
      "Epoch 19/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8085\n",
      "Epoch 20/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8115\n",
      "Epoch 21/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4244 - accuracy: 0.8110\n",
      "Epoch 22/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4216 - accuracy: 0.8120\n",
      "Epoch 23/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8130\n",
      "Epoch 24/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4183 - accuracy: 0.8125\n",
      "Epoch 25/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4163 - accuracy: 0.8120\n",
      "Epoch 26/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4142 - accuracy: 0.8120\n",
      "Epoch 27/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4128 - accuracy: 0.8120\n",
      "Epoch 28/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8140\n",
      "Epoch 29/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4082 - accuracy: 0.8150\n",
      "Epoch 30/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4063 - accuracy: 0.8160\n",
      "Epoch 31/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8160\n",
      "Epoch 32/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.4017 - accuracy: 0.8185\n",
      "Epoch 33/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8225\n",
      "Epoch 34/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8235\n",
      "Epoch 35/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8265\n",
      "Epoch 36/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8290\n",
      "Epoch 37/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8345\n",
      "Epoch 38/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8335\n",
      "Epoch 39/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8345\n",
      "Epoch 40/40\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.3815 - accuracy: 0.8370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f39aced570>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.fit(X_train, y_train, batch_size=32, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test The Neural Network\n",
    "Once trained, we can use the .predict() method to a single obsevation and an array of observations.\\\n",
    "The model will return a array of arrays, in the nested arrays are the values of the output nodes.\\\n",
    "in our case there is only 1 output.\\\n",
    "\\\n",
    "After passing the test set through the model, we iterate through the returned values, reseting that index to true if over 0.5, otherwise false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_predict = ann.predict(X_test)\n",
    "y_predict = (y_predict > .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "Scikit_Learn provides us with tool to visualize the results of our model.\\\n",
    "One way to visualize the results is the Confusion Matrix.\\\n",
    "We will pass both the predicted outputs to the actual outputs, and it will return an with a nested array for each possible outcome.\\\n",
    "In our case True or False, so we will get 2 nested arrays.\\\n",
    "In each array there will be 2 values, 1st being how many times it predicted that outcome correctly, 2nd being incorrect guesses of that outcome.\\\n",
    "\\\n",
    "**Example:**\\\n",
    "['Guessed false correctly (n) amount of times', Guessed false incorrectly (n) amount of times'],\\\n",
    "['Guessed true correctly (n) amount of times', Guessed true incorrectly (n) amount of times']\\\n",
    "\\\n",
    "acurracy will compare the two, predicted and actual, and return a float value representing the accuracy rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guessed False 6127 times correctly and 237 time incorrectly\n",
      "Guessed True 1102 times correctly and 534 time incorrectly\n",
      "83% Accuracy\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_predict)\n",
    "print(f'Guessed False {cm[0][0]} times correctly and {cm[0][1]} time incorrectly')\n",
    "print(f'Guessed True {cm[1][0]} times correctly and {cm[1][1]} time incorrectly')\n",
    "print(f'{round(accuracy_score(y_test, y_predict)*100)}% Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n",
      "Probabilty of customer leaving: 33%\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Customer will stay\n"
     ]
    }
   ],
   "source": [
    "# data  = [[1,0,0,600,1,40,3,60000,2,1,1,50000]]\n",
    "data  = [[0,0,1,700,1,60,1,0,1,1,1,101000]]\n",
    "\n",
    "data = sc.transform(data)\n",
    "\n",
    "print(f'Probabilty of customer leaving: {round(ann.predict(data)[0][0]*100)}%')\n",
    "print('Customer will leave') if ann.predict(data)[0][0] > .5 else print('Customer will stay')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
