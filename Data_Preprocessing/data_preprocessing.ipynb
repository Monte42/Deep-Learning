{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for converting or minipulating the data\n",
    "import matplotlib.pyplot as plt # for plotting the data on graph to see\n",
    "import pandas as pd # for importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tools that we will use as we advance.\\\n",
    "These are the basic tools for now so we can focus more understanding what we are doing and why.\\\n",
    "In fact there are libraries that we will use to help in the preprocessing stage.\\\n",
    "More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "Here we imported that Data.csv and stored it in Variable called dataset.\\\n",
    "What csv_read returns is called a dataframe.\\\n",
    "A Dataframe is a 2 dimensional data structure, like a 2 dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any dataset, that you will train a model with, you will have two disticnt entities.\\\n",
    "The set a features, which are the known characteristics of the observation.\\\n",
    "Or the independent variable(s).\\\n",
    "Then the set of dependent variable.\\\n",
    "\\\n",
    "Lets take a look at our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here 4 colums, location, age, income, and if the bought a product.\\\n",
    "Real quick we can make out that if a person purchased a product is less likey to affect the other three columns.\\\n",
    "While any of the other may affect if someone does buy that product.\\\n",
    "We can safely say that (Country, Age, Salary) are the features and (Purchased) is the independent variable.\\\n",
    "We also have two cells with missing data.  More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE .values turns the dataframe to a numpy array\n",
    "x = dataset.iloc[:,:-1].values # All Rows, All Columns except the last / feature set / independent variables\n",
    "y = dataset.iloc[:,-1].values # All Rows, Only last column / output vecter / dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, dataset is a pandas dataframe that represents a tabel of data.\\\n",
    "Its not an array, but for now lets think of it as an array of arrays.\\\n",
    "We know in Python we index in a range like so: arr[ start : end : step ]\\\n",
    "We also know [ -1 ] is the same as indexing the last element.\\\n",
    "Well, what we might not know is that we can't index a dataframe like an array: dataset[ : ][ : -1 ]\\\n",
    "The Pandas dataframe frame has a built in method for indexing data called iloc.\\\n",
    "Pandas.iloc[  ] requires 1 argument but can take 2. indexing of the first dimension and then the second.\\\n",
    "Indexing values can bet both a range or a single index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "So as we seen above there was some data missing in our dataset.\\\n",
    "There a couple of ways to handle missing data:\n",
    "- First if the dataset is very large and we are only missing a small %, we can just delete rows with missing data.\n",
    "- A second way, and the way we'll do it is, replace the data with the avg of all the rows in that column of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are add to out Data Preprocessing Tools.\\\n",
    "Normally we would import this with the rest of the imports at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we imported SimpleImputer is a class of sklearn library.\\\n",
    "Next we create a variable to store our new class obj and pass in a couple of arguments.\\\n",
    "First is missing_values, by pass np.nan we are basically saying and cell that doesnt have a value, or \"not a number\"\n",
    "\\\n",
    "SimpleImputer also can do more than replace empty vaules with an average.\\\n",
    "You can also do things like the median, or most common value if its categorical like Country.\\\n",
    "\\\n",
    "NOTE: this set up is for numerical values, we will only want to apply it to the age and salary columns.\\\n",
    "For good practice, when doing this include all numerical columns, as we wont really know where there is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(x[:,1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:,1:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the cells are full and the two oddballs are very obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "### Encoding The Independent Variable\n",
    "Looking at our dataset, most of the data is a numerical value which is good.\\\n",
    "Our learning models will tend to have difficulty finding correlation with strings and the output vector.\\\n",
    "Which is why we are going to encode categorical data such as Country.\\\n",
    "You may think that we would just asign countries numerical values,\\\n",
    "but we dont want to give the impression that there is a ordering relation between countries.\\\n",
    "In other words Franch isn't first, Spain isn't second and so on.\\\n",
    "\\\n",
    "The Solution we will use is \"one hot encoding\".\\\n",
    "This is were instead of giving a numerical value to cells with \"Germany\" we give each unique entry its own column.\\\n",
    "Then in this case, a 1 will be placed in it respective country, while the others get a 0.\\\n",
    "So instead of it being:\n",
    "- France = 0\n",
    "- Spain = 1\n",
    "- Germany = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it would be:\n",
    "- France = [1,0,0,...]\n",
    "- Spain = [0,1,0,...]\n",
    "- Germany = [0,0,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "x = np.array(ct.fit_transform(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets talk about the above code quick.\\\n",
    "The first two line could be added to our imports at the top of the file.\\\n",
    "Sklearn is a very popular machine learning library that has many powerful data preproccesing tools.\\\n",
    "\\\n",
    "First we import ColumnTransformer, A class from sklearn that can be used to update the vecter to inclued the new cells.\\\n",
    "Next we grabbed OneHotEncoder, this is the tool that we can use to convert \"Country\" into a vecter's for more efficient learning.\\\n",
    "We create a variable and store an instance of the ColumnTransformer class.\\\n",
    "ct will need a few arguments passed to it:\n",
    "- First - Type of transformer we need, the string 'encoder' is an accepted parameter that tells ct we want to encode the data.\n",
    "- Second - The method to be used for the transformation - we want OneHotEncoder() so we pass just that.\n",
    "- Third - What to do with data not changed - remainder='passthrough' - this say to leave the data there, the defalut is to drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass x into ct.fit_transform() to covnert the feature set into the new set with the new cols.\\\n",
    "The ct.fit_transform does not return a numpy array, so the last line we just make sure that x is converted back to a numpy array.\\\n",
    "\\\n",
    "Real quick lets look at the new dataset, we can the first col has been replaced with three new ones containing either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoding The Dependent Variable\n",
    "In our case the dependent variable is categorical, and we are not worried about the ordering concept as before.\\\n",
    "Meaning like the \"Country\" col, it contains strings, so we will want to convert them to numerical values.\\\n",
    "Unlike \"Country\" on the other hand, we can asign each value its own numerical value.\\\n",
    "\\\n",
    "So the concept is the same, but instead of giving each option its own column like before,\\\n",
    "we will a \"yes\" the value of 1, and \"no\" the value 0.\\\n",
    "\\\n",
    "Since we aren't adding new cols we wont need ColumnTransformer like before.\\\n",
    "We will also not use OneHotEncoder, but instead LableEncoder beacause we are only working one col.\\\n",
    "We create a variable to store the LabelEncoder instance, and this time no arguments are required.\\\n",
    "We did not convert y into a numpy array, because it wont actually be passed through the machine learning model.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we successfully converted the dependent variable vector into numerical data the compter can now understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling And Splitting The Data\n",
    "There has been some discussion on if we should split the data before or after the feature scaling.\\\n",
    "I would like to take a minute and explain why we do it before we scale the features.\\\n",
    "\\\n",
    "When we get our raw dataset it can have all kinds of values stored in it.\\\n",
    "Up to now we converted categorized data into values of 1's and 0's,\\\n",
    "and filled in the empty cells, but we could have some colums with a large difference in values.\\\n",
    "For example, think of age and salary.  Age is <100 while salary is in the 1000's.\\\n",
    "\\\n",
    "The data we pass to the training model will need each column changed so all the values are in a comparable range.\\\n",
    "Lets say values between -3/+3, or 0/1.\\\n",
    "\\\n",
    "We are going to talk about 2 of the main types of feature scaling.\\\n",
    "First I want to say I'm not a mathematician, but I will explain the best way I know how.\\\n",
    "We are about to see an x a single quote above it, that is signifying \"The new cell value of this iteration of x in an array of x's\".\\\n",
    "Also its very import to remeber, scaling is column based.\n",
    "### Normalization\n",
    "> #### $X^{'}=\\frac{x-x_{min}}{x_{max}-x_{min}}$\n",
    "This is the Normalization formular used for scaling our data.\\\n",
    "Whats being said here is, the new value of x is the current value of x minus the min of whole list divided by the list max minus the list min.\\\n",
    "Lets apply this to a few exaples in the age column of our dataset.\\\n",
    "- MIN: 27\n",
    "- MAX: 50\n",
    ">#### $X^{'}=\\frac{27-27}{50-27}=\\frac{0}{23}=0.0$\n",
    "In this example we entered the min age into the formula, and 0 was the answer\n",
    ">#### $X^{'}=\\frac{50-27}{50-27}=\\frac{23}{23}=1.0$\n",
    "In this example we entered the max age into the formula, and we got 1 for the answer\n",
    ">#### $X^{'}=\\frac{38-27}{50-27}=\\frac{11}{23}=0.478$\n",
    "Here I grabbed a random age of 38. We got a value that was between 1 and 0.\\\n",
    "This will work with any age in the data set. It also works the same on the salary column.\n",
    "### Standardization\n",
    "> #### $X^{'}=\\frac{x-\\mu}{\\sigma}$\n",
    "Here is our standardization formula.\\\n",
    "This one will be a little harder to explain.\\\n",
    "Here, new value of X is the current value of X minus the mean(average) of the list divided by the standard deviation.\\\n",
    "So what is the \"Standard Deviation\"?\\\n",
    "Its a measure of how dispersed the data is in relation to its mean.\\\n",
    "Mathematically, it's the square root of the ((sum of ((each value minus the mean) squared)) divided by the number of values in the dataset).\n",
    "> #### $\\sigma=\\sqrt{\\frac{\\Sigma(x^{i}-\\mu)^{2}}{N}}$\n",
    "So now, our formula looks more like this.\\\n",
    "so first we solve for standard deviation, then solve for $X^{'}$\n",
    "> #### $X^{'}=\\frac{x-\\mu}{\\sqrt{\\frac{\\Sigma(x^{i}-\\mu)^{2}}{N}}}$\n",
    "Needless to say I wont be running examples of this one in this section.\\\n",
    "\\\n",
    "By now, I'm sure you're wondering why go through all this?\\\n",
    "Well, as mentioned before in our dataset the age column and salary columns have very different size values.\\\n",
    "We don't want the much larger integer values of the salary over power the age column.\\\n",
    "Lets look at a visual example.\\\n",
    "![DataPrep](./img/data_prep(1).png)\\\n",
    "If you were asked to group row 2 with, or who 2 was closer to on a general scale who would you pick?\\\n",
    "\\\n",
    "Well, lets first look at the differences and the compare.\\\n",
    "![DataPrep](./img/data_prep(2).png)\\\n",
    "We can see the salary difference between row 1 and 2 is $10,000 while row 2 and 3 is $8,000.\\\n",
    "If we compare the differences theres a gap of 2000.\\\n",
    "For age the diffence from 1 and 2 is 1, while the diffence from 2 and three is 4.\\\n",
    "This gives a gap of 3.\\\n",
    "So, we want to say person 2 is closer to person 3 because 2000 os so much larger than 3.\\\n",
    "\\\n",
    "Now lets see what happens after we apply normalization.\\\n",
    "![DataPrep](./img/data_prep(3).png)\\\n",
    "That looks different!\\\n",
    "Now we can easliy see that person sits about in the middle as far as salary is concerned.\\\n",
    "Age on the otherhand, is much closer to person 1.\\\n",
    "So now we can clearly and confidently group person 2 with person 1.\\\n",
    "The same applies to our learning model.\n",
    "### Ok so which do we preform first, split the data or feature scaling?\n",
    "Now that we understand feature scaling, we can tackle this question.\\\n",
    "The idea of a test set is to have data that have both the independent and dependent variables.\\\n",
    "This way we can enter single observations, get the predicted outcome, and compare it to what actually happend.\\\n",
    "\\\n",
    "As we now know when we apply feature scaling, we are working with min's, max's or mean's.\\\n",
    "This means if our test data can contaminate the feature scaling results.\\\n",
    "This is actually called \"data leakage\", remember we don't want our model to have any contact with the test data.\\\n",
    "Another thing to consider is once the model is deployed other than handeling categorical data, we won't scale new data.\\\n",
    "\\\n",
    "I think now the answer is obvious which has to be done first now.  Which is, splitting the data.\n",
    "### Split Data Into Train/Test Sets\n",
    "The next step is to split the data into 2 sets.\\\n",
    "A set to train our learning model with and another much smaller set to test with.\\\n",
    "\\\n",
    "Well, actually 4 different sets od data.\\\n",
    "We still have the train\\test split concept, but in reality each set has to split into 2 seperate matrixs.\\\n",
    "A set for independent variables/featurs/\"y\", and a set for the dependent variables or \"x\".\\\n",
    "\\\n",
    "Good news, sklearn has a tool for that, it will split our data into the 4 matrixs that we need.\\\n",
    "This means we will have x_train, x_test, y_train, y_test sets all made for us.\\\n",
    "\\\n",
    "**Why do we to break our data into four sets again?**\\\n",
    "Our learning model will be expecting both the y_train and x_train to train with, and both test sets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was painless! Lets cover what we just did.\\\n",
    "We import the train_test_split method from sklearn.\\\n",
    "train_test_split will return for elements. In Python, when assigning variables seperated by commas, as above, is called unpacking.\\\n",
    "So, line 2 is unpacking the 4 elements returned from the method, and assigning to variables with realitive names.\\\n",
    "train_test_split, exects a couple arguments, plus we set a couple extra:\n",
    "- First - we need to pass in the list of the features\n",
    "- Second - we pass in all the dependent variables \n",
    "- Third - is how much of the dataset we want to reserve for testing.  Most people view an 80/20 split to be best.\n",
    "- Fourth - When the data is being split, there are random factors that pick what data goes into to the test set. This just ensures the results are the same everytime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "X Train\n",
      "=======\n",
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "=======\n",
      "X Test\n",
      "=======\n",
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "=======\n",
      "Y Train\n",
      "=======\n",
      "[0 1 0 0 1 1 0 1]\n",
      "=======\n",
      "Y Test\n",
      "=======\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(f'=======\\nX Train\\n=======\\n{x_train}')\n",
    "print(f'=======\\nX Test\\n=======\\n{x_test}')\n",
    "print(f'=======\\nY Train\\n=======\\n{y_train}')\n",
    "print(f'=======\\nY Test\\n=======\\n{y_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "When scalling our dataset, should we scale the the data we already minipulated? (Categorical Data)\n",
    "The answer is no, for two reason:\n",
    "- That data we already set to be represented a value(s) of of 0's and 1's, so it's arealdy in the deisired range.\n",
    "- Changing that data may alter its representation.  It does not help improve our results, it may even hurt them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n",
      " [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n",
      " [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n",
      " [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n",
      " [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n",
      " [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n",
      " [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n",
      " [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n",
      "[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n",
      " [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# sc = StandardScaler()\n",
    "# x_train[:,-2:] = sc.fit_transform(x_train[:,-2:]) \n",
    "# x_test[:,-2:] = sc.transform(x_test[:,-2:])\n",
    "\n",
    "sc = MinMaxScaler(feature_range=(0,1), copy=True) # these are default and dont need to add\n",
    "x_train = sc.fit_transform(x_train) # Dont need to parse data before scaling\n",
    "x_test = sc.fit_transform(x_test)\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets recap whats going on in the above cell.\\\n",
    "The first line we import both StandardScaler(standardization), and the MinMaxScaler(normalization) classes.\\\n",
    "The next 3 lines we standardize the data, then the 3 following lines we normalize the data.\\\n",
    "\\\n",
    "To standardize the data we first create an instance of the StandardScaler class and store it a variable. No arguments are needed.\\\n",
    "Next we apply the the StandardScaler to both our x_traind and x_test sets.\\\n",
    "Remember we are going to apply it to the \"dummy data\", or categorical data we already altered, which includes both y sets.\\\n",
    "\\\n",
    "Now, lets break these lines down.\\\n",
    "The first 3 columns of our feature set is the \"Country\" representaion, so we do want to risk altering it.\\\n",
    "In Python if we have nested array we can index both axis's in single sqaure brackets by seperating the index calls by a comma.\\\n",
    "x_train[:,-2:] - from x_train list [from every row, select all from the second to last colum to the last column].\\\n",
    "We set this equal to StandardScaler.fit_transform, and pass x_train as an argument.\\\n",
    "This say's, take the last 2 cols, standardize and update the dataset while keeping the rest of the dataset.\\\n",
    "\\\n",
    "Cool, lets standardize the test set. NOTE: that we use StandardScaler.fit.\\\n",
    "This is because when we call fit_transform we build the scaler to be applied in the test application.\\\n",
    "When we just call fit, we apply the scaler that was built during training.\\\n",
    "Thats its the data is standaradized.\\\n",
    "\\\n",
    "The final 3 lines are the same concept, but normaliztion.\\\n",
    "Both have their advantages in different situations.\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
