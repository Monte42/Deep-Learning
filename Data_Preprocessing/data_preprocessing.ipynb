{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for converting or minipulating the data\n",
    "import matplotlib.pyplot as plt # for plotting the data on graph to see\n",
    "import pandas as pd # for importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tools that we will use as we advance.\\\n",
    "These are the basic tools for now so we can focus more understanding what we are doing and why.\\\n",
    "In fact there are libraries that we will use to help in the preprocessing stage.\\\n",
    "More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "Here we imported that Data.csv and stored it in Variable called dataset.\\\n",
    "What csv_read returns is called a dataframe.\\\n",
    "A Dataframe is a 2 dimensional data structure, like a 2 dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any dataset, that you will train a model with, you will have two disticnt entities.\\\n",
    "The set a features, which are the known characteristics of the observation.\\\n",
    "Or the independent variable(s).\\\n",
    "Then the set of dependent variable.\\\n",
    "\\\n",
    "Lets take a look at our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here 4 colums, location, age, income, and if the bought a product.\\\n",
    "Real quick we can make out that if a person purchased a product is less likey to affect the other three columns.\\\n",
    "While any of the other may affect if someone does buy that product.\\\n",
    "We can safely say that (Country, Age, Salary) are the features and (Purchased) is the independent variable.\\\n",
    "We also have two cells with missing data.  More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE .values turns the dataframe to a numpy array\n",
    "x = dataset.iloc[:,:-1].values # All Rows, All Columns except the last / feature set / independent variables\n",
    "y = dataset.iloc[:,-1].values # All Rows, Only last column / output vecter / dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, dataset is a pandas dataframe that represents a tabel of data.\\\n",
    "Its not an array, but for now lets think of it as an array of arrays.\\\n",
    "We know in Python we index in a range like so: arr[ start : end : step ]\\\n",
    "We also know [ -1 ] is the same as indexing the last element.\\\n",
    "Well, what we might not know is that we can't index a dataframe like an array: dataset[ : ][ : -1 ]\\\n",
    "The Pandas dataframe frame has a built in method for indexing data called iloc.\\\n",
    "Pandas.iloc[  ] requires 1 argument but can take 2. indexing of the first dimension and then the second.\\\n",
    "Indexing values can bet both a range or a single index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "So as we seen above there was some data missing in our dataset.\\\n",
    "There a couple of ways to handle missing data:\n",
    "- First if the dataset is very large and we are only missing a small %, we can just delete rows with missing data.\n",
    "- A second way, and the way we'll do it is, replace the data with the avg of all the rows in that column of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are add to out Data Preprocessing Tools.\\\n",
    "Normally we would import this with the rest of the imports at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we imported SimpleImputer is a class of sklearn library.\\\n",
    "Next we create a variable to store our new class obj and pass in a couple of arguments.\\\n",
    "First is missing_values, by pass np.nan we are basically saying and cell that doesnt have a value, or \"not a number\"\n",
    "\\\n",
    "SimpleImputer also can do more than replace empty vaules with an average.\\\n",
    "You can also do things like the median, or most common value if its categorical like Country.\\\n",
    "\\\n",
    "NOTE: this set up is for numerical values, we will only want to apply it to the age and salary columns.\\\n",
    "For good practice, when doing this include all numerical columns, as we wont really know where there is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(x[:,1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:,1:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the cells are full and the two oddballs are very obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "### Encoding The Independent Variable\n",
    "Looking at our dataset, most of the data is a numerical value which is good.\\\n",
    "Our learning models will tend to have difficulty finding correlation with strings and the output vector.\\\n",
    "Which is why we are going to encode categorical data such as Country.\\\n",
    "You may think that we would just asign countries numerical values,\\\n",
    "but we dont want to give the impression that there is a ordering relation between countries.\\\n",
    "In other words Franch isn't first, Spain isn't second and so on.\\\n",
    "\\\n",
    "The Solution we will use is \"one hot encoding\".\\\n",
    "This is were instead of giving a numerical value to cells with \"Germany\" we give each unique entry its own column.\\\n",
    "Then in this case, a 1 will be placed in it respective country, while the others get a 0.\\\n",
    "So instead of it being:\n",
    "- France = 0\n",
    "- Spain = 1\n",
    "- Germany = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it would be:\n",
    "- France = [1,0,0,...]\n",
    "- Spain = [0,1,0,...]\n",
    "- Germany = [0,0,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "x = np.array(ct.fit_transform(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets talk about the above code quick.\\\n",
    "The first two line could be added to our imports at the top of the file.\\\n",
    "Sklearn is a very popular machine learning library that has many powerful data preproccesing tools.\\\n",
    "\\\n",
    "First we import ColumnTransformer, A class from sklearn that can be used to update the vecter to inclued the new cells.\\\n",
    "Next we grabbed OneHotEncoder, this is the tool that we can use to convert \"Country\" into a vecter's for more efficient learning.\\\n",
    "We create a variable and store an instance of the ColumnTransformer class.\\\n",
    "ct will need a few arguments passed to it:\n",
    "- First - Type of transformer we need, the string 'encoder' is an accepted parameter that tells ct we want to encode the data.\n",
    "- Second - The method to be used for the transformation - we want OneHotEncoder() so we pass just that.\n",
    "- Third - What to do with data not changed - remainder='passthrough' - this say to leave the data there, the defalut is to drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass x into ct.fit_transform() to covnert the feature set into the new set with the new cols.\\\n",
    "The ct.fit_transform does not return a numpy array, so the last line we just make sure that x is converted back to a numpy array.\\\n",
    "\\\n",
    "Real quick lets look at the new dataset, we can the first col has been replaced with three new ones containing either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoding The Dependent Variable\n",
    "In our case the dependent variable is categorical, and we are not worried about the ordering concept as before.\\\n",
    "Meaning like the \"Country\" col, it contains strings, so we will want to convert them to numerical values.\\\n",
    "Unlike \"Country\" on the other hand, we can asign each value its own numerical value.\\\n",
    "\\\n",
    "So the concept is the same, but instead of giving each option its own column like before,\\\n",
    "we will a \"yes\" the value of 1, and \"no\" the value 0.\\\n",
    "\\\n",
    "Since we aren't adding new cols we wont need ColumnTransformer like before.\\\n",
    "We will also not use OneHotEncoder, but instead LableEncoder beacause we are only working one col.\\\n",
    "We create a variable to store the LabelEncoder instance, and this time no arguments are required.\\\n",
    "We did not convert y into a numpy array, because it wont actually be passed through the machine learning model.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we successfully converted the dependent variable vector into numerical data the compter can now understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data Into Train/Test Sets\n",
    "The next step is to split the data into 2 sets.\\\n",
    "A set to train our learning model with and another much smaller set to test with.\\\n",
    "\\\n",
    "If you wondering why we are splitting the data before we scale you can refer to [Scaling vs Splitting](../Notes/Data_Preprocessing/scaling_vs_spliting.ipynb)\\\n",
    "\\\n",
    "Well, actually 4 different sets od data.\\\n",
    "We still have the train\\test split concept, but in reality each set has to split into 2 seperate matrixs.\\\n",
    "A set for independent variables/featurs/\"y\", and a set for the dependent variables or \"x\".\\\n",
    "\\\n",
    "Good news, sklearn has a tool for that, it will split our data into the 4 matrixs that we need.\\\n",
    "This means we will have x_train, x_test, y_train, y_test sets all made for us.\\\n",
    "\\\n",
    "**Why do we to break our data into four sets again?**\\\n",
    "Our learning model will be expecting both the y_train and x_train to train with, and both test sets for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was painless! Lets cover what we just did.\\\n",
    "We import the train_test_split method from sklearn.\\\n",
    "train_test_split will return for elements. In Python, when assigning variables seperated by commas, as above, is called unpacking.\\\n",
    "So, line 2 is unpacking the 4 elements returned from the method, and assigning to variables with realitive names.\\\n",
    "train_test_split, exects a couple arguments, plus we set a couple extra:\n",
    "- First - we need to pass in the list of the features\n",
    "- Second - we pass in all the dependent variables \n",
    "- Third - is how much of the dataset we want to reserve for testing.  Most people view an 80/20 split to be best.\n",
    "- Fourth - When the data is being split, there are random factors that pick what data goes into to the test set. This just ensures the results are the same everytime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======\n",
      "X Train\n",
      "=======\n",
      "[[0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 35.0 58000.0]]\n",
      "=======\n",
      "X Test\n",
      "=======\n",
      "[[0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n",
      "=======\n",
      "Y Train\n",
      "=======\n",
      "[0 1 0 0 1 1 0 1]\n",
      "=======\n",
      "Y Test\n",
      "=======\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "print(f'=======\\nX Train\\n=======\\n{x_train}')\n",
    "print(f'=======\\nX Test\\n=======\\n{x_test}')\n",
    "print(f'=======\\nY Train\\n=======\\n{y_train}')\n",
    "print(f'=======\\nY Test\\n=======\\n{y_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "When scalling our dataset, should we scale the the data we already minipulated? (Categorical Data)\n",
    "The answer is no, for two reason:\n",
    "- That data we already set to be represented a value(s) of of 0's and 1's, so it's arealdy in the deisired range.\n",
    "- Changing that data may alter its representation.  It does not help improve our results, it may even hurt them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.         0.51207729 0.11428571]\n",
      " [0.         1.         0.         0.56521739 0.45079365]\n",
      " [1.         0.         0.         0.73913043 0.68571429]\n",
      " [0.         0.         1.         0.47826087 0.37142857]\n",
      " [0.         0.         1.         0.         0.        ]\n",
      " [1.         0.         0.         0.91304348 0.88571429]\n",
      " [0.         1.         0.         1.         1.        ]\n",
      " [1.         0.         0.         0.34782609 0.28571429]]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "sc = StandardScaler()\n",
    "x_train[:,-2:] = sc.fit_transform(x_train[:,-2:]) \n",
    "x_test[:,-2:] = sc.transform(x_test[:,-2:])\n",
    "\n",
    "# sc = MinMaxScaler(feature_range=(0,1), copy=True) # these are default and dont need to add\n",
    "# x_train = sc.fit_transform(x_train) # Dont need to parse data before scaling\n",
    "# x_test = sc.fit_transform(x_test)\n",
    "\n",
    "print(x_train)\n",
    "print(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets recap whats going on in the above cell.\\\n",
    "The first line we import both StandardScaler(standardization), and the MinMaxScaler(normalization) classes.\\\n",
    "The next 3 lines we standardize the data, then the 3 following lines we normalize the data.\\\n",
    "\\\n",
    "To standardize the data we first create an instance of the StandardScaler class and store it a variable. No arguments are needed.\\\n",
    "Next we apply the the StandardScaler to both our x_traind and x_test sets.\\\n",
    "Remember we are going to apply it to the \"dummy data\", or categorical data we already altered, which includes both y sets.\\\n",
    "\\\n",
    "Now, lets break these lines down.\\\n",
    "The first 3 columns of our feature set is the \"Country\" representaion, so we do want to risk altering it.\\\n",
    "In Python if we have nested array we can index both axis's in single sqaure brackets by seperating the index calls by a comma.\\\n",
    "x_train[:,-2:] - from x_train list [from every row, select all from the second to last colum to the last column].\\\n",
    "We set this equal to StandardScaler.fit_transform, and pass x_train as an argument.\\\n",
    "This say's, take the last 2 cols, standardize and update the dataset while keeping the rest of the dataset.\\\n",
    "\\\n",
    "Cool, lets standardize the test set. NOTE: that we use StandardScaler.fit.\\\n",
    "This is because when we call fit_transform we build the scaler to be applied in the test application.\\\n",
    "When we just call fit, we apply the scaler that was built during training.\\\n",
    "Thats its the data is standaradized.\\\n",
    "\\\n",
    "The final 3 lines are the same concept, but normaliztion.\\\n",
    "Things to note are we dont have to index like before, and the arguments are defaulted to these settings.\\\n",
    "Both standardization and normalization have their advantages in different situations.\\\n",
    "\\\n",
    "With that our data is ready for our learning models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
