{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for converting or minipulating the data\n",
    "import matplotlib.pyplot as plt # for plotting the data on graph to see\n",
    "import pandas as pd # for importing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more tools that we will use as we advance.\\\n",
    "These are the basic tools for now so we can focus more understanding what we are doing and why.\\\n",
    "In fact there are libraries that we will use to help in the preprocessing stage.\\\n",
    "More on that later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset\n",
    "Here we imported that Data.csv and stored it in Variable called dataset.\\\n",
    "What csv_read returns is called a dataframe.\\\n",
    "A Dataframe is a 2 dimensional data structure, like a 2 dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any dataset, that you will train a model with, you will have two disticnt entities.\\\n",
    "The set a features, which are the known characteristics of the observation.\\\n",
    "Or the independent variable(s).\\\n",
    "Then the set of dependent variable.\\\n",
    "\\\n",
    "Lets take a look at our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "5   France  35.0  58000.0       Yes\n",
      "6    Spain   NaN  52000.0        No\n",
      "7   France  48.0  79000.0       Yes\n",
      "8  Germany  50.0  83000.0        No\n",
      "9   France  37.0  67000.0       Yes\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here 4 colums, location, age, income, and if the bought a product.\\\n",
    "Real quick we can make out that if a person purchased a product is less likey to affect the other three columns.\\\n",
    "While any of the other may affect if someone does buy that product.\\\n",
    "We can safely say that (Country, Age, Salary) are the features and (Purchased) is the independent variable.\\\n",
    "We also have two cells with missing data.  More on that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE .values turns the dataframe to a numpy array\n",
    "x = dataset.iloc[:,:-1].values # All Rows, All Columns except the last / feature set / independent variables\n",
    "y = dataset.iloc[:,-1].values # All Rows, Only last column / output vecter / dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, dataset is a pandas dataframe that represents a tabel of data.\\\n",
    "Its not an array, but for now lets think of it as an array of arrays.\\\n",
    "We know in Python we index in a range like so: arr[ start : end : step ]\\\n",
    "We also know [ -1 ] is the same as indexing the last element.\\\n",
    "Well, what we might not know is that we can't index a dataframe like an array: dataset[ : ][ : -1 ]\\\n",
    "The Pandas dataframe frame has a built in method for indexing data called iloc.\\\n",
    "Pandas.iloc[  ] requires 1 argument but can take 2. indexing of the first dimension and then the second.\\\n",
    "Indexing values can bet both a range or a single index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Missing Data\n",
    "So as we seen above there was some data missing in our dataset.\\\n",
    "There a couple of ways to handle missing data:\n",
    "- First if the dataset is very large and we are only missing a small %, we can just delete rows with missing data.\n",
    "- A second way, and the way we'll do it is, replace the data with the avg of all the rows in that column of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are add to out Data Preprocessing Tools.\\\n",
    "Normally we would import this with the rest of the imports at the top of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we imported SimpleImputer is a class of sklearn library.\\\n",
    "Next we create a variable to store our new class obj and pass in a couple of arguments.\\\n",
    "First is missing_values, by pass np.nan we are basically saying and cell that doesnt have a value, or \"not a number\"\n",
    "\\\n",
    "SimpleImputer also can do more than replace empty vaules with an average.\\\n",
    "You can also do things like the median, or most common value if its categorical like Country.\\\n",
    "\\\n",
    "NOTE: this set up is for numerical values, we will only want to apply it to the age and salary columns.\\\n",
    "For good practice, when doing this include all numerical columns, as we wont really know where there is missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(x[:,1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:,1:3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see all the cells are full and the two oddballs are very obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data\n",
    "### Encoding The Independent Variable\n",
    "Looking at our dataset, most of the data is a numerical value which is good.\\\n",
    "Our learning models will tend to have difficulty finding correlation with strings and the output vector.\\\n",
    "Which is why we are going to encode categorical data such as Country.\\\n",
    "You may think that we would just asign countries numerical values,\\\n",
    "but we dont want to give the impression that there is a ordering relation between countries.\\\n",
    "In other words Franch isn't first, Spain isn't second and so on.\\\n",
    "\\\n",
    "The Solution we will use is \"one hot encoding\".\\\n",
    "This is were instead of giving a numerical value to cells with \"Germany\" we give each unique entry its own column.\\\n",
    "Then in this case, a 1 will be placed in it respective country, while the others get a 0.\\\n",
    "So instead of it being:\n",
    "- France = 0\n",
    "- Spain = 1\n",
    "- Germany = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it would be:\n",
    "- France = [1,0,0,...]\n",
    "- Spain = [0,1,0,...]\n",
    "- Germany = [0,0,1,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "x = np.array(ct.fit_transform(x)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets talk about the above code quick.\\\n",
    "The first two line could be added to our imports at the top of the file.\\\n",
    "Sklearn is a very popular machine learning library that has many powerful data preproccesing tools.\\\n",
    "\\\n",
    "First we import ColumnTransformer, A class from sklearn that can be used to update the vecter to inclued the new cells.\\\n",
    "Next we grabbed OneHotEncoder, this is the tool that we can use to convert \"Country\" into a vecter's for more efficient learning.\\\n",
    "We create a variable and store an instance of the ColumnTransformer class.\\\n",
    "ct will need a few arguments passed to it:\n",
    "- First - Type of transformer we need, the string 'encoder' is an accepted parameter that tells ct we want to encode the data.\n",
    "- Second - The method to be used for the transformation - we want OneHotEncoder() so we pass just that.\n",
    "- Third - What to do with data not changed - remainder='passthrough' - this say to leave the data there, the defalut is to drop it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass x into ct.fit_transform() to covnert the feature set into the new set with the new cols.\\\n",
    "The ct.fit_transform does not return a numpy array, so the last line we just make sure that x is converted back to a numpy array.\\\n",
    "\\\n",
    "Real quick lets look at the new dataset, we can the first col has been replaced with three new ones containing either 1 or 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.0 0.0 44.0 72000.0]\n",
      " [1.0 0.0 0.0 1.0 27.0 48000.0]\n",
      " [1.0 0.0 1.0 0.0 30.0 54000.0]\n",
      " [1.0 0.0 0.0 1.0 38.0 61000.0]\n",
      " [1.0 0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [0.0 1.0 0.0 0.0 35.0 58000.0]\n",
      " [1.0 0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [0.0 1.0 0.0 0.0 48.0 79000.0]\n",
      " [1.0 0.0 1.0 0.0 50.0 83000.0]\n",
      " [0.0 1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Encoding The Dependent Variable\n",
    "In our case the dependent variable is categorical, and we are not worried about the ordering concept as before.\\\n",
    "Meaning like the \"Country\" col, it contains strings, so we will want to convert them to numerical values.\\\n",
    "Unlike \"Country\" on the other hand, we can asign each value its own numerical value.\\\n",
    "\\\n",
    "So the concept is the same, but instead of giving each option its own column like before,\\\n",
    "we will a \"yes\" the value of 1, and \"no\" the value 0.\\\n",
    "\\\n",
    "Since we aren't adding new cols we wont need ColumnTransformer like before.\\\n",
    "We will also not use OneHotEncoder, but instead LableEncoder beacause we are only working one col.\\\n",
    "We create a variable to store the LabelEncoder instance, and this time no arguments are required.\\\n",
    "We did not convert y into a numpy array, because it wont actually be passed through the machine learning model.\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we successfully converted the dependent variable vector into numerical data the compter can now understand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data Into Train/Test Sets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
